Extreme Learning Machine
========================

During the last few years, the Extreme Learning Machine (ELM) got a lot of attention.
Shark does not have a class tailored to ELMs but supports all building blocks to create one.

In theory an ELM is not more than a simple Feed Forward neural network where the hidden
weights are not trained. For the ELM, we need the following include files: ::

..sharkcode<Supervised/elmTutorial.tpp,includes>
  
To prevent that single inputs with high variance dominate the behaviour of the ELM, the inputs are required to 
have zero mean and unit variance. So we need a model which applies this transformation to the data. 
This is a linear transformation which can be learned by an affine linear model using the proper trainer::

..sharkcode<Supervised/elmTutorial.tpp,normalization>
  
Now we can create the ELM. We first create an FFNet with a single hidden layer and initialize 
all its weights randomly. We will keep the random weights of the hidden units but overwrite 
the output weights later::

..sharkcode<Supervised/elmTutorial.tpp,FFNetStructure>

Since the output weights of the ELM are linear, we can now use linear regression to train them. 
The best way to do is, is to propagate the training data forward through the normalization 
and hidden layer and use the outputs as input data for the linear regression. After the linear regression
is done, we set the weights of the second layer of the FFNet to the linear weights::

..sharkcode<Supervised/elmTutorial.tpp,train>

After training is done, the last thing we have to do is concatenate the normalizer with the network
and present our shiny new elm to the world! ::
  
..sharkcode<Supervised/elmTutorial.tpp,elm>

Full example program
--------------------

The full example program is  :doxy:`elmTutorial.cpp`.
